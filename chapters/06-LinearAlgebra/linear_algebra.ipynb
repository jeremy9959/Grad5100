{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Essential Linear Algebra\n",
        "subtitle: Fundamentals of Data Science\n",
        "author: Jeremy Teitelbaum\n",
        "format: html\n",
        "title-block-style: plain\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Arrow\n",
        "\n",
        "\n",
        "def make_plot(xmin, ymin, xmax, ymax):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(xmin, xmax)\n",
        "    ax.set_ylim(ymin, ymax)\n",
        "    ax.set_xticks(np.arange(xmin, xmax, 1))\n",
        "    ax.set_yticks(np.arange(ymin, ymax, 1))\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.grid(visible=True)\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def draw_arrow(x0, y0, x1, y1, axes, color=\"blue\", alpha=1):\n",
        "    axes.add_patch(Arrow(x0, y0, x1 - x0, y1 - y0, width=0.3, color=color, alpha=alpha))\n",
        "    return axes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R and Python\n",
        "\n",
        "For a look at linear algebra basics in R and Python, see:\n",
        "\n",
        "1. [Python Linear Algebra](linear_algebra_python.qmd)\n",
        "2. [R Linear Algebra](linear_algebra_r.qmd)\n",
        "\n",
        "\n",
        "## Vectors and Scalars\n",
        "\n",
        "$\\mathbf{R}^{n}$ is the set of vectors (ordered tuples) of real numbers of length $n$.\n",
        "A *scalar* is a real number.\n",
        "\n",
        "- Vectors are *added* componentwise.\n",
        "- A vector can be multiplied by a scalar.\n",
        "\n",
        "\n",
        "## Addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = make_plot(0, 0, 12, 12)\n",
        "ax.set_title(\"Vector Addition\")\n",
        "ax = draw_arrow(0, 0, 3, 5, ax)\n",
        "ax = draw_arrow(0, 0, 1, 4, ax)\n",
        "ax = draw_arrow(1, 4, 4, 9, ax)\n",
        "ax = draw_arrow(3, 5, 4, 9, ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scalar Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = make_plot(0, 0, 12, 12)\n",
        "ax.set_title(\"Scalar Multiplication\")\n",
        "ax = draw_arrow(0, 0, 3, 5, ax)\n",
        "ax = draw_arrow(0, 0, 6, 10, ax, color=\"red\", alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geometric Interpretation in 2 and 3 dimensions\n",
        "\n",
        "## Feature Space \n",
        "\n",
        "Each 'dimension' in feature space corresponds to a 'feature' or measurement\n",
        "of the data.  *Here we are assuming for now that the features are continuous\n",
        "and measured by real numbers*.\n",
        "\n",
        "Let's choose some numerical features of the penguins dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv(\"data/penguins-raw.csv\")\n",
        "data = (\n",
        "    data[\n",
        "        [\n",
        "            \"Culmen Length (mm)\",\n",
        "            \"Culmen Depth (mm)\",\n",
        "            \"Flipper Length (mm)\",\n",
        "            \"Body Mass (g)\",\n",
        "        ]\n",
        "    ]\n",
        "    .dropna()\n",
        "    .values\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each penguin is represented by a vector in $\\mathbf{R}^{4}$.  So for example penguin number 34 is represented as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data[34, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This abstraction of penguins into vectors is sometimes called \"an embedding\".\n",
        "\n",
        "## Features\n",
        "\n",
        "We can also look at a single feature for *all* of the penguins.  For example,\n",
        "'Culmen Length (mm)' is a feature and there is a vector in $\\mathbf{R}^{342}$\n",
        "consisting of all of the Culmen Lengths for all of the penguins.\n",
        "\n",
        "In the *tidy* convention, we summarize our data in an array or matrix where each row corresponds to a sample and each column to a feature.   So our penguin\n",
        "data has $342$ rows (corresponding to the 342 penguins with no missing data)\n",
        "and 4 columns corresponding to four features. \n",
        "\n",
        "## Image Embeddings\n",
        "\n",
        "Each sample in the MNIST database is a $28x28$ gray scale image, represented by\n",
        "a $28\\times 28$ array of integers between 0 and 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with open(\"data/train-images.idx3-ubyte\", \"rb\") as f:\n",
        "    f.read(16)\n",
        "    buf = f.read(28 * 28)\n",
        "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "    data = data.reshape(1, 28, 28)\n",
        "for x in range(28):\n",
        "    for y in range(28):\n",
        "        print(\"{:>4}\".format(int(data[0, x, y])), end=\"\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "image = np.asarray(data[0].squeeze())\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can view each image as a vector in a 784 dimensional (=28*28) space. \n",
        "\n",
        "A collection of 100 images would be represented by an array with 100 \n",
        "rows and 784 columns\n",
        "\n",
        "A 28x28 image in color has 28*28*3 numbers to account for the RGB channels.\n",
        "\n",
        "## One Hot Embedding\n",
        "\n",
        "Normally categorical variables don't embed direcly into $\\mathbf{R}^{n}$\n",
        "but one can use \"one-hot\" embedding.\n",
        "\n",
        "Suppose our categorical vector has 4 levels: red, green, blue, orange.\n",
        "\n",
        "The \"one-hot\" embedding uses four features, and each color corresponds\n",
        "to a vector with a one in the column corresponding to the color and zeros elsewhere.\n",
        "\n",
        "| red | green | blue | orange |\n",
        "|---|---|---|---|\n",
        "|1 |0 |0 |0|\n",
        "|0 |1|0|0|\n",
        "|0|0|1|0|\n",
        "|0|0|0|1|\n",
        "\n",
        "So to use one-hot encoding of our feature data, we'd add four columns to our data\n",
        "matrix.\n",
        "\n",
        "## Word embeddings\n",
        "\n",
        "Word2vec is a technique developed by scientists at google that embeds\n",
        "a vocabulary into $\\mathbf{R}^{n}$. Each of 3 million words has\n",
        "a 300 dimensional vector representing it.\n",
        "\n",
        "See [this google page.](https://code.google.com/archive/p/word2vec/)\n",
        "\n",
        "\n",
        "## Linear combinations\n",
        "\n",
        "If $v_1,\\ldots, v_k$ are vectors in $\\mathbf{R}^{n}$ then a weighted sum of the\n",
        "$v_{i}$ is called a *linear combination.*\n",
        "\n",
        "$$\n",
        "w = \\sum b_{i}v_{i}\n",
        "$$\n",
        "\n",
        "Suppose our data is the performance of students on 2 homeworks, 1 midterm,\n",
        "and one final, all scored on a 100 points scale, with each homework worth 10% of the total, the midterm worth 25% and the final worth 55%.  If there are 20 students\n",
        "our data is a $20 x 4$ array with each row having the grades of a single\n",
        "student and each column having all the scores for a particular assignment.\n",
        "\n",
        "Let $v_1, v_2, v_3, v_4$ be the four columns.  Then the final score is the linear\n",
        "combination\n",
        "$$\n",
        "s=.1v_1+.1v_2+.25v_3+.55v_4\n",
        "$$\n",
        "\n",
        "The vector $s$, the *score*, is a linear combination of the features.\n",
        "\n",
        "\n",
        "## Linear Dependence and Linear Independence\n",
        "\n",
        "In the example above, the final score is a linear combination of the features.\n",
        "We say that the final score is *dependent* on the features.  More generally,\n",
        "a collection of vectors is linearly *dependent* if there are constants,\n",
        "not all zero, so that\n",
        "\n",
        "$$\n",
        "b_1v_1+...+b_kv_k=0.\n",
        "$$\n",
        "\n",
        "If they're dependent, it means one of them can be written in terms of the other.\n",
        "\n",
        "If they aren't dependent, they are *independent.*  This means that the only\n",
        "way you can get \n",
        "$$\n",
        "b_1v_1+...+b_kv_k=0\n",
        "$$\n",
        "is if all the constants are zero.\n",
        "\n",
        "None of the vectors can be written in terms of the others. \n",
        "\n",
        "Mathematically, dependence is an *exact* relationship. \n",
        "\n",
        "## Linear relations in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectors = [np.random.uniform(size=8) for i in range(5)]  # 10 random vectors\n",
        "scalars = [np.random.normal() for i in range(5)]  # 10 random scalars\n",
        "prods = [scalars[i] * vectors[i] for i in range(5)]  # products\n",
        "result = sum(prods)  # sum of products\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approximate linear dependence\n",
        "\n",
        "Sometimes two features are \"almost\" linearly related.  \n",
        "\n",
        "In an old dataset about car models, with 398 types of cars and 9 features,\n",
        "two features are \"miles per gallon\" and \"engine displacement\".  If we look\n",
        "at mpg and displacement relative to their means by subtracting their averages values, we see that \n",
        "\n",
        "\n",
        "Here, miles per gallon (relative to its mean value)\n",
        "is roughly linearly dependent on displacement (relative to its mean).\n",
        "$$\n",
        "\\mathrm{mpg}-\\overline{\\mathrm{mpg}} = -.0603(\\mathrm{disp}-\\overline{\\mathrm{disp}})\n",
        "$$\n",
        "\n",
        "So we don't learn much new from 'mpg' that isn't already in 'displacement'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "mpg = pd.read_csv(\"data/auto-mpg.csv\")\n",
        "x = mpg[\"displacement\"].values\n",
        "x = x - np.mean(x)\n",
        "y = mpg[\"mpg\"].values\n",
        "y = y - np.mean(y)\n",
        "x1 = sm.add_constant(x)\n",
        "model = sm.OLS(y, x1).fit()\n",
        "x0 = np.linspace(-100, 300, 10)\n",
        "x1 = sm.add_constant(x0)\n",
        "predictions = model.predict(x1)\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x0, predictions, color=\"red\")\n",
        "plt.title(\"Miles per Gallon vs Engine Displacement\")\n",
        "plt.xlabel(\"Displacement\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear Regression (studied later) tries to capture this. \n",
        "\n",
        "## Span\n",
        "\n",
        "The span of a collection of vectors is the set of all linear combinations\n",
        "of those vectors. \n",
        "\n",
        "\n",
        "## Basis and dimension\n",
        "\n",
        "A basis is a linearly independent, spanning set. The number of elements\n",
        "in a basis is always the same; it is called the *dimension* of the vector space. \n",
        "\n",
        "The dimension of $\\mathbf{R}^{n}$ is $n$ (the standard vectors are independent and span).\n",
        "\n",
        "## Distances and the euclidean norm\n",
        "\n",
        "The norm of a vector $v=(a_1,\\ldots, a_n)$ is \n",
        "$$\n",
        "\\|v\\| = (\\sum a_{i}^2)^{1/2}\n",
        "$$\n",
        "\n",
        "It is the \"length\" of the vector.  \n",
        "\n",
        "The Euclidean distance between two points $v$ and $w$ is\n",
        "$$\n",
        "\\|v-w\\|\n",
        "$$\n",
        "\n",
        "## Mean Squared Error in vector form\n",
        "\n",
        "Remember our example the \"almost\" dependence of mpg and displacement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mpg0 = mpg[\"mpg\"] - np.mean(mpg[\"mpg\"])\n",
        "disp0 = mpg[\"displacement\"] - np.mean(mpg[\"displacement\"])\n",
        "predicted = -0.0603 * disp0\n",
        "plt.scatter(disp0, mpg0)\n",
        "plt.scatter(disp0, predicted)\n",
        "plt.title(\"MSE={:.2f}\".format(np.linalg.norm(mpg0 - predicted) / mpg0.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean squared error is the squared distance between a true and predicted value, divided by the number of values. \n",
        "\n",
        "\n",
        "## The dot product\n",
        "\n",
        "Suppose we have two vectors:\n",
        "$$\\begin{aligned}\n",
        "v_1 & =[a_1,a_2,\\dots, a_n] \\\\\n",
        "v_2 &= [b_1,b_2,\\ldots, b_n]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The \"dot product\" or \"inner product\" of these two vectors is:\n",
        "$$\n",
        "v_1\\cdot v_2 = \\sum_{i=1}^{n} a_i b_i.\n",
        "$$\n",
        "\n",
        "*Important:* The dot product *converts two vectors into a scalar!\n",
        "\n",
        "## Properties of the dot product\n",
        "\n",
        "1. $v_1\\cdot v_1 = \\|v_1\\|^2$\n",
        "2. $(av_1+bv_2)\\cdot v_3 = a(v_1\\cdot v_2) + b(v_2\\cdot v_2)$\n",
        "3. $v_1\\cdot v_2 = v_2 \\cdot v_1$.\n",
        "\n",
        "## Angles and Cauchy-Schwartz\n",
        "\n",
        "The law of cosines:\n",
        "$$\\|v_1\\|^2+\\|v_2\\|^2 - 2\\|v_1\\|\\|v_2\\|\\cos(\\theta) =\\|v_1-v_2\\|^2$$\n",
        "\n",
        "means that\n",
        "\n",
        "$$\n",
        "v_1\\cdot v_2 = \\|v_1\\|\\|v_2\\|\\cos(\\theta)\n",
        "$$ {#eq-dot-product}\n",
        "\n",
        "In particular:\n",
        "\n",
        "$$|v_1\\cdot v_2|\\le \\|v_1\\|\\|v_2\\|\n",
        "$$ {#eq-cauchy-schwartz}\n",
        "\n",
        "This says\n",
        "$$\n",
        "|\\sum_{i=1}^{n} a_{i}b_{i}|^2\\le (\\sum_{i=1}^{n} a_{i}^2)(\\sum_{i=1}^{n} b_{i}^2)\n",
        "$$\n",
        "\n",
        "@eq-cauchy-schwartz is called the \"Cauchy-Schwartz inequality.\"\n",
        "\n",
        "## Python\n",
        "\n",
        "```{#lst-dot-product .python lst-cap=\"Dot product computation\"}\n",
        "# python\n",
        "v = np.array([1,2,3,4,5])\n",
        "w = np.array([2,4,6,8,10])\n",
        "print('Entry by entry product = {}'.format(v*w)) # <1>\n",
        "print('Dot product = {}'.format(np.dot(v,w))) # <2>\n",
        "```\n",
        "\n",
        "Note: In R, the symbol for dot product is\n",
        "`%*%`.\n",
        "\n",
        "In @lst-dot-product we show how to compute the dot product in python. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Orthogonality\n",
        "\n",
        "If $v_1\\cdot v_2=0$ then either\n",
        "one of $v_1$ or $v_2$ is zero, or\n",
        "the angle between then is 90 degrees.\n",
        "\n",
        "In this case we saw the vectors are *orthogonal*.\n",
        "\n",
        "## Unit vectors and projection\n",
        "\n",
        "A vector $u$ is a unit vector if $u\\cdot u=1$.\n",
        "\n",
        "The quantity $v\\cdot u$ measures the projection of $v$\n",
        "into the direction given by $u$. \n",
        "\n",
        "\n",
        "## Variance, Correlation and cosine similarity\n",
        "\n",
        "If $v$ is a feature vector, let \n",
        "$\\overline{v}=\\frac{1}{n}\\sum_{i=1}^{n} v_{i}$.\n",
        "\n",
        "Notice that $\\overline{v}=v\\cdot E$ where $E$\n",
        "is the vector with $1/n$ in each entry. \n",
        "\n",
        "$$\n",
        "\\|(v-\\overline{v}E)\\|=\\sigma^{2}\n",
        "$$\n",
        "\n",
        "\n",
        "## Covariance\n",
        "\n",
        "If $v$ and $w$ are two vectors, their *covariance*\n",
        "is\n",
        "$$\n",
        "\\sigma_{vw} = (v-\\overline{v}E)\\cdot (w-\\overline{w}E)\n",
        "$$\n",
        "\n",
        "## Correlation\n",
        "\n",
        "The correlation coefficient of $v$ and $w$ is\n",
        "$$\n",
        "r_{vw}^2 = \\frac{\\sigma_{vw}}{\\sigma{v}\\sigma{w}}\n",
        "$$\n",
        "\n",
        "It measures the cosine of the angle between $v-\\overline{v}E$\n",
        "and $w-\\overline{w}E$.  \n",
        "\n",
        "## Cosine similarity\n",
        "\n",
        "In general, \n",
        "\n",
        "$$\n",
        "\\cos(\\theta) = \\frac{v\\cdot w}{\\|v\\|\\|w\\|}\n",
        "$$\n",
        "measures the angle between two feature vectors and is a measure\n",
        "of \"similarity\" between $0$ and $1$.  \n",
        "\n",
        "## Hyperplanes\n",
        "\n",
        "A (linear) hyperplane is a subspace of dimension $n-1$ in a vector\n",
        "space of dimension $n$.  It is given by an equation of the form\n",
        "\n",
        "$$\n",
        "\\sum a_{i}x_{i}=0.\n",
        "$$\n",
        "\n",
        "Geometrically this can be written $v\\cdot x=0$ where\n",
        "$v=[a_1,\\ldots, a_n]$ and $x=[x_1,\\ldots, x_n]$.\n",
        "The vector $v$ is called the *normal* vector to the hyperplane.\n",
        "\n",
        "An (affine) hyperplane is given by an equation of the form\n",
        "$$\n",
        "v\\cdot x = b\n",
        "$$\n",
        "for some constant $b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.linspace(-5, 5, 10)\n",
        "y = 2 / 3 * x\n",
        "plt.plot(x, y)\n",
        "plt.grid(True)\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "\n",
        "plt.plot([0, -2], [0, 3])\n",
        "plt.plot([-2, -2], [3, 2])\n",
        "plt.plot([-2, -1], [3, 2.6])\n",
        "plt.title(\"Hyperplane 2x-3y=0 with normal vector [-2,3]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Affine hyperplanes\n",
        "\n",
        "For fixed $v$ and varying $b$, the hyperplanes\n",
        "$v\\cdot x=b$ form a parallel family.\n",
        "\n",
        "## Distance from a point to a hyperplane\n",
        "\n",
        "The distance from a point $w$ to the hyperplane $v\\cdot x = b$ is\n",
        "$$\n",
        "D = \\frac{w\\cdot v-b}{\\|v\\|}.\n",
        "$$\n",
        "\n",
        "This is the projection of the line from $w$ to a point $x$ on the hyperplane against the unit normal $\\frac{v}{\\|v\\|}$. \n",
        "\n",
        "$$\n",
        "(w-x)\\cdot\\frac{v}{\\|v\\|} =\\frac{(w\\cdot v -x\\cdot v)}{\\|v\\|} = \\frac{(w\\cdot v - b)}{\\|v\\|}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.linspace(-4, 4, 10)\n",
        "y = 2 / 3 * x\n",
        "plt.plot(x, y)\n",
        "plt.grid(True)\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.title(\"Dist. from point to hyperplane\")\n",
        "plt.plot([0, -2 / np.sqrt(13)], [0, 3 / np.sqrt(13)], color=\"red\", linewidth=3)\n",
        "plt.plot([0, -2], [0, 3])\n",
        "plt.plot([-3, -2], [-2, 3])\n",
        "plt.text(-2.4, 3, r\"w\")\n",
        "plt.text(-3, -2.3, r\"x\")\n",
        "plt.text(-3.6, 0, r\"$\\|w-x\\|$\")\n",
        "plt.text(-1.9, 2.2, r\"$\\theta$\")\n",
        "plt.text(-0.5, 1.4, r\"$\\|w-x\\|\\cos(\\theta)$\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrices \n",
        "\n",
        "An $n\\times m$ matrix is an array of real numbers\n",
        "with $n$ rows, $m$ columns, and a total of $nm$ entries. \n",
        "\n",
        "## Matrix times Vector\n",
        "\n",
        "If $M$ is an $n\\times m$ matrix and $v$ is\n",
        "an $m\\times 1$ vector (a column vector) then\n",
        "$Mv$ is the $n\\times 1$ column vector whose\n",
        "entries are\n",
        "$$\n",
        "M[i,:]\\cdot v\n",
        "$$\n",
        "where $i$ runs from 1 to $n$.  Here $M[i,:]$ is\n",
        "the $i^{th}$ row of $M$. \n",
        "\n",
        "If $v$ is a $1\\times n$ row vector then\n",
        "$vM$ is the $1\\times m$ row vector whose entries\n",
        "are $v\\cdot M[:,i]$ as $i$ runs from 1 to $m$. \n",
        "\n",
        "## Matrix times vector is linear\n",
        "\n",
        "$$M(v+w) = Mv+Mw$$\n",
        "$$M(aw) = aMw$$\n",
        "\n",
        "## Matrix times standard vector gives row/column\n",
        "\n",
        "If $v$ is a column vector with a  a $1$ in position $i$ and zeros elsewhere, then $Mv$ is the $i^{th}$ column of $M$.\n",
        "\n",
        "If $v$ is a row vector with a $1$ in position\n",
        "$i$ and zeros elsewhere, then $vM$ is the $i^{th}$\n",
        "row of $M$.\n",
        "\n",
        "## $Mv$/$vM$ gives linear combination of columns/rows\n",
        "\n",
        "$Mv$ is a linear combination of the columns of $M$\n",
        "weighted by the entries of $v$.\n",
        "\n",
        "$vM$ is a linear combination of the rows of $M$ weighted by the entries of $v$.\n",
        "\n",
        "## Matrix times matrix\n",
        "\n",
        "An $n\\times m$ matrix times an $m\\times k$ matrix yields an $n\\times k$\n",
        "matrix.\n",
        "\n",
        "You can view $MN$ as $Mv$ where $v$ runs through the columns of $N$.  Each column has $n$ rows.\n",
        "\n",
        "$$\n",
        "MN=\\left[\n",
        "    \\begin{matrix} \n",
        "     M[0,:]N & M[1,:]N &\\cdots&M[m,:]N\\\\\n",
        "    \\end{matrix} \n",
        "    \\right]\n",
        "$$\n",
        "\n",
        "OR  you can view $MN$ as $wN$ where\n",
        "$w$ runs through the rows of $M$.\n",
        "\n",
        "$$\n",
        "MN = \\left[\\begin{matrix} MN[:,0] \\\\ MN[:,1] \\\\\\vdots\\\\MN[:,k]\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "OR\n",
        "\n",
        "you can view $MN$ where\n",
        "the $i,j$ entry of $MN$ is the dot product\n",
        "of the $i^{th}$ row of $M$ with the $j^{th}$ column of $N$ (each of which has $m$ entries).\n",
        "\n",
        "$$(MN)_{ij} = M[i,:]\\cdot N[;,j]$$\n",
        "\n",
        "## Python\n",
        "\n",
        "The '@' sign gives matrix multiplication in python.\n",
        "In R, it's %*%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "M = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5]])\n",
        "v = np.array([[-1], [0], [1]])\n",
        "print(M)\n",
        "print(v)\n",
        "print(M @ v)\n",
        "print(\"Compare -M[:,0]+M[:,2] with M@v\\n both are {}\".format(-M[:, 0] + M[:, 2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where does matrix multiplication come from?\n",
        "\n",
        "If $v$ is in $\\mathbf{R}^{m}$ as a column vector, and $M$ is an $n\\times m$\n",
        "matrix, then $Mv\\in\\mathbf{R}^{n}$.  So the function $v\\to Mv$ is a function\n",
        "from $\\mathbf{R}^{m}\\to \\mathbf{R}^{m}$.\n",
        "\n",
        "Now if $N$ is a $k\\times n$ matrix, then $NMv$ is in $\\mathbf{R}^{k}$.\n",
        "\n",
        "If we want $N(Mv)=(NM)v$ to be true then this forces the definition of matrix multiplication.  \n",
        "\n",
        "**The Matrix Product gives composition of functions**\n",
        "\n",
        "\n",
        "## Transpose of a matrix\n",
        "\n",
        "The transpose $M^{T}$ of a matrix is the matrix obtained from $M$ by switching rows and columns.\n",
        "\n",
        "The transpose switches the order of a product. \n",
        "\n",
        "$$(MN)^{T}=N^{T}M^{T}$$\n",
        "\n",
        "\n",
        "## Covariance Matrix\n",
        "\n",
        "Remember that if $v$ and $w$ are feature vectors, then\n",
        "$(v-\\overline{v})\\cdot (w-\\overline{w})$ is the covariance of $v$ and $w$.\n",
        "\n",
        "Suppose $v_1,\\ldots, v_n$ are features forming a data matrix $X$.\n",
        "\n",
        "Let $X_{0}$ be the matrix whose columns are $v_{i}-\\overline{v_{i}}$.\n",
        "\n",
        "Then $\\frac{1}{N}X_{0}^{T}X_{0}$ is $n\\times n$ and called the *covariance matrix*.\n",
        "\n",
        "If $Y_{0}$ is obtained from $X_{0}$ by dividing each column by its norm,\n",
        "then $Y_{0}^{T}Y_{0}$ is the correlation matrix -- ones on the diagonal,\n",
        "correlation coefficients off diagonal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv(\"data/penguins-raw.csv\")\n",
        "data = (\n",
        "    data[\n",
        "        [\n",
        "            \"Culmen Length (mm)\",\n",
        "            \"Culmen Depth (mm)\",\n",
        "            \"Flipper Length (mm)\",\n",
        "            \"Body Mass (g)\",\n",
        "        ]\n",
        "    ]\n",
        "    .dropna()\n",
        "    .values\n",
        ")\n",
        "# axis=0 means take the average of the columns (summarize over rows)\n",
        "data.mean(axis=0)\n",
        "\n",
        "# \"center\" each column; scale column 3\n",
        "data0 = data - data.mean(axis=0)\n",
        "print(np.linalg.norm(data0, axis=0))\n",
        "data0 = data0 / np.linalg.norm(data0, axis=0)\n",
        "D = data0.transpose() @ data0\n",
        "print(D)\n",
        "plt.imshow(D, cmap=\"hot\", interpolation=\"nearest\")\n",
        "plt.title(\"covariance matrix heatmap\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.scatter(x=data0[:, 2], y=data0[:, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rank and invertibility\n",
        "\n",
        "The column rank of a matrix is the dimension of the\n",
        "space spanned by its columns; this is the number\n",
        "of linearly independent columns.\n",
        "\n",
        "The row rank is the number of linearly independent rows. \n",
        "\n",
        "**Theorem:** These two numbers are equal.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "base",
      "language": "python",
      "display_name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}